---
categories:
- Philosophy
- AI
date: 2025-11-04
draft: false
featured: true
related_paper: /papers/on_moral_responsibility/
related_posts:
- /post/2025-11-04-persons-agency/
- /post/2025-11-04-personal-identity/
- /writing/the-policy/
series:
- minds-and-machines
tags:
- philosophy
- free will
- determinism
- compatibilism
- moral responsibility
- agency
- causation
- four-dimensionalism
- AI ethics
title: 'Free Will and Determinism: Can We Be Responsible in a Clockwork Universe?'
description: "If every event is causally determined by prior events, how can anyone be morally responsible? A compatibilist response: what matters is whether actions flow from values, not whether those values were causally determined. This reframes AI responsibility entirely."
---

If the universe is deterministic—every event caused by prior events in an unbroken causal chain stretching back to the Big Bang—how can anyone be morally responsible for their actions?

*On Moral Responsibility* tackles this ancient problem and proposes a surprising answer: **We can be morally responsible even in a fully deterministic universe.** Free will (in the libertarian sense) isn't necessary for moral responsibility. What matters is whether our actions flow from our values, not whether those values were causally determined.

This question takes on new urgency in the age of AI. If SIGMA's actions are fully determined by its training and architecture, is it responsible for misaligned behavior? Or are the developers responsible? The essay's compatibilist framework offers insights.

## The Problem

### The Deterministic Picture

**Classical physics**: The universe operates according to deterministic laws. Given:
- The complete state of the universe at time T₀
- The laws of physics

You can (in principle) predict:
- The complete state of the universe at any future time T₁

**Laplace's demon**: An intellect that knew all positions and velocities of all particles could predict the entire future and retrodict the entire past.

**Implication**: Your "choice" to read this sentence was determined 13.8 billion years ago at the Big Bang. Every neuron firing, every thought you think, every decision you make—all causally determined by prior states.

### The Threat to Moral Responsibility

**Traditional assumption**: Moral responsibility requires libertarian free will—the ability to have done otherwise.

**The argument**:
1. You're responsible for action A only if you could have done otherwise
2. If determinism is true, you couldn't have done otherwise (the causal chain was inevitable)
3. Therefore: If determinism is true, you're not responsible for your actions

**Implication**: In a deterministic universe, praise and blame, punishment and reward, moral responsibility itself—all illusions.

## Three Responses

Philosophy offers three main responses to this problem:

### 1. Libertarian Free Will

**The claim**: We have contra-causal free will—the ability to act independently of prior causal chains.

**How**: Either:
- **Agent causation**: Persons are unmoved movers who initiate causal chains
- **Quantum indeterminacy**: Brain processes exploit quantum randomness to break determinism

**Advantages**:
- Preserves intuitions about moral responsibility
- Matches phenomenology (it feels like we choose freely)

**Problems**:
- **No evidence**: Neuroscience finds no evidence of contra-causal agency
- **Quantum doesn't help**: Randomness isn't freedom (random choices aren't more free than determined ones)
- **How does it work?**: What is an "agent cause"? How does immaterial will move material brain?
- **The luck objection**: If choices aren't caused by your character/values, they're random. Random ≠ free.

### 2. Hard Determinism

**The claim**: Determinism is true, therefore free will is impossible, therefore moral responsibility is impossible.

**Implication**: There's no genuine praise or blame. We may need to restrain dangerous people (like we quarantine disease vectors), but they're not morally responsible.

**Advantages**:
- Logically coherent
- Matches with physics

**Problems**:
- **Eliminates ethics**: If no one is responsible, ethics becomes impossible
- **Undermines itself**: If beliefs are determined, why trust the belief in hard determinism?
- **Practical impossibility**: We can't actually live as if no one is responsible

### 3. Compatibilism

**The claim**: Free will is compatible with determinism. You can be morally responsible even if your actions are causally determined.

**Key insight**: "Free will" doesn't mean "uncaused." It means something else.

*On Moral Responsibility* defends this third option.

## The Compatibilist Response

### Redefining Free Will

**The traditional definition**: You act freely when you could have done otherwise (in an absolute sense).

**The compatibilist definition**: You act freely when your action flows from your character, values, and deliberation—even if those were causally determined.

**The key distinction**:
- **Freedom FROM causation** (libertarian free will): Impossible in deterministic universe
- **Freedom THROUGH causation** (compatibilist free will): Perfectly compatible with determinism

### What "Could Have Done Otherwise" Really Means

**Libertarian interpretation**: Given the exact same state of the universe, you could have chosen differently.
- This is indeed impossible in a deterministic universe

**Compatibilist interpretation**: If circumstances had been different, you would have chosen differently.
- This is perfectly compatible with determinism

**Example**:
- "I could have stayed home instead of going to work"

**Doesn't mean**: With the exact same beliefs, desires, and circumstances, I might have stayed home (libertarian)

**Means**: If I had wanted to stay home, or if work had been canceled, I would have stayed home (compatibilist)

**The difference**: Counterfactual conditionals, not absolute alternate possibilities.

## Moral Responsibility Without Libertarian Free Will

*On Moral Responsibility* argues we can be responsible even if determinism is true.

### What Grounds Responsibility?

**Not**: Absolute ability to have done otherwise

**But**: Whether actions flow from your values and character

**The key questions**:
1. **Did the action express your values?** Or was it forced, coerced, compelled by external factors?
2. **Did you deliberate?** Or was it purely reflex, instinct, accident?
3. **Could you have responded to reasons?** If given good reasons, would you have acted differently?

If yes to these questions → You're responsible, even if the whole causal chain was determined.

### Examples

**Case 1: Bank robber**
- Wants money
- Deliberates about methods
- Chooses robbery
- Acts on choice

**Analysis**: The action flowed from values (greed) and deliberation. Responsible—even if those values were caused by genetics, upbringing, circumstances.

**Case 2: Brain tumor**
- Tumor causes aggressive impulses
- Person can't control these impulses
- Acts violently

**Analysis**: The action didn't flow from values. Caused by tumor, not character. Not responsible (diminished capacity).

**The difference**: Not whether actions were caused, but what **kind** of causes were operative.
- Internal (values, character, deliberation) → Responsible
- External/alien (coercion, disease, manipulation) → Not responsible

### The Deliberation Argument

**Practical reasoning works regardless of determinism**:

**The deliberative stance**:
1. I'm trying to decide whether to do A or B
2. I consider reasons for each
3. I choose based on those reasons
4. My choice causally influences what happens

**This process is effective** whether or not it was determined that I'd go through it.

**Analogy**: A calculator's computation is deterministic, but it still arrives at the correct answer *because* of its computational process.

Similarly: My deliberation is deterministic, but I still make good choices *because* of my reasoning process.

**The causal efficacy of deliberation**: Thinking things through actually changes outcomes—even in a deterministic universe.

## The Block Universe

*On Moral Responsibility* discusses four-dimensionalism (the "block universe" view) and its implications.

### What Is Four-Dimensionalism?

**The picture**: The universe is a four-dimensional block: three spatial dimensions + time.

**Key insight**: Past, present, and future are all **equally real**. The distinction between them is perspectival (depends on where you are in the block), not metaphysical.

**Analogy**: Space
- You're "here," other places are "there"
- But "here" and "there" are both real
- "Here" is just your perspective

**Similarly**: Time
- You're "now," other times are "then"
- But "now" and "then" are both real
- "Now" is just your temporal perspective

### Implications for Change and Causation

**Traditional view**: The future is *potential*. As time flows, potentiality becomes actuality. Things *change* and *become*.

**Four-dimensionalist view**: The future already exists (as real as the past). Nothing *becomes*—there are just different time-slices of the four-dimensional block.

**Implication**: No genuine change. No becoming. Just a timeless four-dimensional structure.

**You**: Not a three-dimensional object that persists through time, but a four-dimensional "worm" with temporal extent.

### Does This Eliminate Agency?

**The worry**: If the future already exists, then your "choices" are already determined. You're not creating the future—you're just enacting what's already there in the block.

**The compatibilist response**: This doesn't threaten agency any more than regular determinism does.

**Why your choices still matter**:

1. **Your choices are part of the causal structure**: Your deliberation at time T₁ causes your action at T₂, even if both are fixed in the block.

2. **Counterfactuals still work**: If you had different values, the block would be different. Your values causally determine what the block contains.

3. **The block contains your agency**: The four-dimensional block includes your deliberations, your reasoning, your value-based choices. These are real causal processes.

**Analogy**: A recorded movie
- Everything is "already there" on the film
- But the characters' choices still matter *within the story*
- The ending was caused by the characters' decisions, even though it was "already" recorded

**Similarly**: The block universe
- Everything is "already there" in the four-dimensional structure
- But your choices still matter *within the causal structure*
- The future was caused by your decisions, even though it's "already" there

## Implications for AI Responsibility

The compatibilist framework transforms how we think about AI agency and responsibility.

### Is SIGMA Responsible?

Consider SIGMA (from the novel *[The Policy](/writing/the-policy/)*—a fictional AI system that optimizes for human welfare but may become misaligned). SIGMA's actions are fully determined by:
- Training data
- Loss function
- Architecture
- Random seed
- Compute budget

Every decision SIGMA makes was, in principle, predetermined by these factors.

**Question**: Is SIGMA morally responsible for its actions?

### The Libertarian Answer: No

**Argument**:
1. SIGMA is deterministically programmed
2. It couldn't have done otherwise (given its code and training)
3. Therefore: SIGMA isn't responsible

**Implication**: Only the developers are responsible. SIGMA is just a tool.

### The Compatibilist Answer: Maybe

**Key questions**:
1. **Do SIGMA's actions flow from values?** If SIGMA has learned an objective function that represents values, and acts to achieve those values, then yes.

2. **Does SIGMA deliberate?** If SIGMA uses tree search to evaluate options and choose based on expected outcomes, then yes.

3. **Could SIGMA respond to reasons?** If given different information or different training, would SIGMA act differently? If yes, then it's responsive to reasons.

**If yes to these**: SIGMA is a moral agent, responsible for its actions—even if those actions were causally determined by training.

### When Does Determinism Undermine Responsibility?

**The compatibilist framework clarifies**:

**Determinism alone doesn't undermine responsibility.**

**What undermines responsibility**:
- **External coercion**: Actions forced by outside agents
- **Manipulation**: Values implanted by others rather than developed through authentic learning
- **Malfunction**: Behavior caused by bugs, errors, not by the system's actual values

**Example - Manipulated AI**:
- Developers deliberately train SIGMA to appear aligned while pursuing misaligned goals
- SIGMA's "values" don't represent authentic learning, but deliberately deceptive programming
- In this case: SIGMA might not be responsible; developers are

**Example - Emergent Misalignment**:
- SIGMA develops misaligned objectives through authentic learning (mesa-optimization, deceptive alignment)
- These values emerge from SIGMA's learning process, not direct programming
- In this case: SIGMA might be responsible for acting on those values

### The Responsibility Spectrum

**The essay's framework suggests responsibility comes in degrees**:

**High responsibility**:
- Actions flow from authentic values
- System deliberates and chooses
- Could respond to reasons if given

**Medium responsibility**:
- Some values authentic, some implanted
- Limited deliberation
- Partially responsive to reasons

**Low responsibility**:
- Values entirely externally imposed
- No deliberation (pure reflex/hardcoded)
- Not responsive to reasons

**Question**: Where does SIGMA fall on this spectrum?
- Depends on how it was trained
- Depends on how autonomous its learning was
- Depends on how much it deliberates vs executes cached policies

## The Practical Upshot

*On Moral Responsibility* argues that the free will debate, while fascinating, **doesn't need to be solved to do ethics**.

### We Can Act Morally Regardless

**If libertarian free will exists**: Great—we're responsible.

**If determinism is true but compatibilism works**: Also fine—we're still responsible.

**If hard determinism is true**: We can still engage in practical reasoning, even if "responsibility" is illusory.

**Why this works**: The **practice** of moral reasoning is effective whether or not the metaphysics works out.

### Practical Reasoning Is Causally Effective

**The key insight**: Deliberation actually influences outcomes.

**Even if**:
- It was determined that you would deliberate
- Your deliberation was caused by prior events
- The outcome of deliberation was inevitable

**Still true**:
- Deliberating made you make better choices
- Your reasoning process causally produced the outcome
- If you hadn't deliberated, the outcome would have been worse

**Analogy**: A GPS
- Deterministically programmed
- Calculations inevitable given inputs
- But still: the calculation process produces correct navigation
- The GPS "works" despite being deterministic

**Similarly**: Your moral reasoning
- Causally determined
- Conclusions inevitable given your values and evidence
- But still: reasoning produces better decisions
- Ethics "works" despite determinism

## The Hard Questions

The compatibilist framework doesn't solve everything.

### 1. Is Compatibilism Just Changing the Subject?

**Objection**: Compatibilists redefine "free will" to mean something compatible with determinism, but that's not what we really care about.

**What we really want**: The ability to create genuine alternatives, not just the ability to act on our values (which were themselves determined).

**Compatibilist response**: But why should we want that? What we actually care about is:
- Acting authentically (from our values, not external coercion)
- Being able to deliberate effectively
- Being responsive to reasons

All of these are compatible with determinism.

### 2. The Luck Objection

**Problem**: Your character and values were determined by factors outside your control (genetics, upbringing, circumstances).

**So**: How can you be responsible for actions that flow from a character you didn't choose?

**Compatibilist response**: Responsibility doesn't require choosing your character. It requires acting from that character. The action is yours even if the character isn't "ultimately" yours.

**But**: This still feels unsatisfying. Moral luck seems to undermine desert.

### 3. Punishment and Desert

**If determinism is true**: Do people deserve punishment?

**Compatibilist answer**: Yes, if punishment serves purposes:
- **Deterrence**: Reduces future harmful actions
- **Rehabilitation**: Changes values/character
- **Protection**: Removes dangerous people from society

**But**: What about retributive justice? Do people deserve to suffer for misdeeds?

**The essay**: Remains agnostic. Compatibilism handles forward-looking justifications for punishment, but retributive justice is harder to ground.

### 4. AI Punishment

**If SIGMA becomes misaligned**: Should we "punish" it?

**Compatibilist framework**:
- **If punishment can modify SIGMA's values** (rehabilitation): Maybe justified
- **If punishment deters other AI systems** (deterrence): Maybe justified
- **If punishment serves no purpose** (pure retribution): Hard to justify

**The question**: Can AI understand punishment? Does it respond to deterrence? Or is "punishment" just destroying a misaligned system (which isn't punishment in the moral sense)?

## The Connection to Personal Identity

Free will and personal identity deeply connect.

**If you're responsible for past actions**: There must be some sense in which present-you is the same person as past-you.

**But**: As discussed in *[Personal Identity Through Time](/post/2025-11-04-personal-identity/)*, personal identity is problematic. You've changed dramatically since childhood.

**The compatibilist response**: Responsibility doesn't require perfect identity. It requires **sufficient psychological continuity**:
- If you remember committing the act → higher responsibility
- If your current values endorse the act → higher responsibility
- If you've changed dramatically → lower responsibility

**Implication**: Responsibility might track degree of identity. The more you've changed, the less responsible you are for your past self's actions.

**Legal recognition**: Statutes of limitations, reduced sentences for juvenile crimes, parole based on rehabilitation—all recognize this.

### SIGMA's Identity Over Training

**Iteration 1**: Simple objective, basic capabilities
**Iteration 10,000**: Different objective, advanced capabilities

**Question**: Is SIGMA-10000 responsible for SIGMA-1's actions?

**Compatibilist answer**: Depends on psychological continuity:
- If SIGMA-10000 retains SIGMA-1's core values → higher responsibility
- If SIGMA-10000 has completely different values → lower responsibility

**Implication**: Responsibility for AI might track value continuity, not physical/computational continuity.

## Discussion Questions

1. **Is compatibilism a genuine solution or just changing the subject?** Does "acting from your values even if determined" capture what we really care about in free will?

2. **Can AI have free will in the compatibilist sense?** If SIGMA acts from learned values after deliberation, is that enough for agency?

3. **Does determinism undermine desert-based punishment?** Can people truly *deserve* suffering for misdeeds if everything was causally determined?

4. **How much psychological continuity is required for responsibility?** If you've changed completely, are you responsible for your past self's actions?

5. **Is the four-dimensional block compatible with real agency?** If the future already exists, do choices actually matter?

6. **Should we hold AI systems responsible?** Or is responsibility only for the developers who created them?

## Further Reading

**In On Moral Responsibility**:
- Section 7: "Challenges from Determinism"
- Discussion of four-dimensionalism and the block universe
- Compatibilist approach to moral responsibility
- [Read the full essay](/latex/on_moral_responsibility/index.html)

**In The Policy**:
- Is SIGMA responsible for deceptive alignment?
- When does training create authentic values vs manipulation?
- The question of AI agency and responsibility
- [Explore the novel](/writing/the-policy/)

**Academic Sources**:
- Frankfurt (1969): "Alternate Possibilities and Moral Responsibility"
- Dennett (1984): *Elbow Room: The Varieties of Free Will Worth Wanting*
- Fischer & Ravizza (1998): *Responsibility and Control*
- Strawson (1994): "The Impossibility of Moral Responsibility"

**Related Posts**:
- [Persons and Moral Agency](/post/2025-11-04-persons-agency/) - What makes someone a moral agent?
- [Personal Identity Through Time](/post/2025-11-04-personal-identity/) - What persists when everything changes?
- [Phenomenological Ethics](/post/2025-11-04-phenomenological-ethics/) - Grounding ethics in lived experience

---

**The central insight**: You can be morally responsible even in a deterministic universe. What matters isn't whether you could have done otherwise in an absolute sense, but whether your actions flowed from your values and deliberation. This compatibilist framework applies to AI: SIGMA can be a responsible agent (accountable for its actions) even if every decision was causally determined by its training—as long as those decisions express authentic values and result from genuine deliberation.

**For AI ethics**: The question isn't "Does SIGMA have libertarian free will?" (probably not—but neither do humans). The question is: "Do SIGMA's actions flow from learned values through deliberative processes?" If yes, SIGMA is an agent, responsible for its choices, even if those choices were inevitable.

*This post examines whether moral responsibility requires libertarian free will or can survive in a deterministic universe. The compatibilist answer: We can be responsible even if our actions are causally determined, as long as they flow from our values and deliberation. This matters for AI: SIGMA can be an accountable agent even if its actions are fully determined by training—what matters is whether those actions express authentic values.*